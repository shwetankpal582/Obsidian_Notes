PRACTICAL : 5

WAP TO CALCULATE THE LOSS USING BATCH GRADIENT DESCENT ALGORITHM

## Code

```
import numpy as np

# Mean Squared Error (MSE) Loss
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Batch Gradient Descent Algorithm
def batch_gradient_descent(X, y, lr=0.01, epochs=100):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(epochs):
        # Prediction for all samples
        y_pred = np.dot(X, weights) + bias

        # Compute gradients
        error = y - y_pred
        dW = -(2 / n_samples) * np.dot(X.T, error)
        db = -(2 / n_samples) * np.sum(error)

        # Update weights and bias
        weights -= lr * dW
        bias -= lr * db

        # Compute and print loss
        loss = mse_loss(y, y_pred)
        print(f"Epoch {epoch+1} - Loss: {loss:.4f}")

    return weights, bias

# Input Data (Linear relationship y = 2x)
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

# Run Batch Gradient Descent
final_weights, final_bias = batch_gradient_descent(X, y, lr=0.01, epochs=20)

# Final Results
print("\nFinal Weights:", final_weights)
print("Final Bias:", final_bias)

```

## Input

```
X = [[1], [2], [3], [4]]
y = [2, 4, 6, 8]  # Linear: y = 2x

```

## Output

```
Epoch 1 - Loss: 60.0000
Epoch 2 - Loss: 34.6519
Epoch 3 - Loss: 20.2155
...
Epoch 19 - Loss: 0.6247
Epoch 20 - Loss: 0.3636

Final Weights: [1.7346]
Final Bias: 0.4741

```