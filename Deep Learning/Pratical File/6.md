PRACTICAL : 6

WAP TO CALCULATE LOSS IN ANN USING CROSS ENTROPY
## Code

```
import numpy as np

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Cross Entropy Loss for binary classification
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15  # to avoid log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Dummy dataset (binary classification)
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y = np.array([[0], [1], [1], [0]])

# Initialize weights and biases
np.random.seed(42)
weights = np.random.randn(2, 1)
bias = np.random.randn(1)

# Forward pass (1-layer ANN with sigmoid)
z = np.dot(X, weights) + bias
y_pred = sigmoid(z)

# Calculate loss
loss = binary_cross_entropy(y, y_pred)

# Show predictions and loss
print("Predicted Outputs (y_pred):\n", y_pred.round(3))
print("Cross Entropy Loss:", round(loss, 4))

```

## Input

```
X = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [[0], [1], [1], [0]]

```

## Output

```
Predicted Outputs (y_pred):
 [[0.405]
 [0.525]
 [0.385]
 [0.504]]
Cross Entropy Loss: 0.7131

```