PRACTICAL : 4

WAP TO CALCULATE THE LOSS USING STOCHASTIC GRADIENT DESCENT ALGORITHM

## Code

```
import numpy as np

# Mean Squared Error (MSE) Loss
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Derivative of MSE Loss w.r.t. prediction
def mse_derivative(y_true, y_pred):
    return -(y_true - y_pred)

# Stochastic Gradient Descent Algorithm
def stochastic_gradient_descent(X, y, lr=0.01, epochs=20):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(epochs):
        for i in range(n_samples):
            xi = X[i]
            yi = y[i]

            # Prediction
            y_pred = np.dot(xi, weights) + bias

            # Compute gradients
            error = mse_derivative(yi, y_pred)
            dW = error * xi
            db = error

            # Update weights and bias
            weights -= lr * dW
            bias -= lr * db

        # Calculate and print loss after each epoch
        y_preds = np.dot(X, weights) + bias
        loss = mse_loss(y, y_preds)
        print(f"Epoch {epoch+1} - Loss: {loss:.4f}")

    return weights, bias

# Input data (Simple Linear Regression)
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])  # Linear relationship: y = 2*x

# Train using SGD
final_weights, final_bias = stochastic_gradient_descent(X, y, lr=0.01, epochs=20)

# Final result
print("\nFinal Weights:", final_weights)
print("Final Bias:", final_bias)

```

## Input

```
X = [[1], [2], [3], [4]]
y = [2, 4, 6, 8]  # Corresponds to y = 2x
```

## Output

```
Epoch 1 - Loss: 1.9949
Epoch 2 - Loss: 1.6603
Epoch 3 - Loss: 1.3822
...
Epoch 19 - Loss: 0.3354
Epoch 20 - Loss: 0.2894

Final Weights: [1.6265]
Final Bias: 0.3763
```
