PRACTICAL : 3

WAP TO CALCULATE THE LOSS USING CROSSÂ ENTROPY

## Code

```
import numpy as np

# Binary Cross Entropy Loss
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15  # to avoid log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return np.mean(loss)

# Categorical Cross Entropy Loss
def categorical_cross_entropy(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = -np.sum(y_true * np.log(y_pred), axis=1)
    return np.mean(loss)

# Binary classification input
y_true_binary = np.array([1, 0, 1, 0])
y_pred_binary = np.array([0.9, 0.1, 0.8, 0.2])

# Categorical classification input
y_true_cat = np.array([
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1]
])
y_pred_cat = np.array([
    [0.7, 0.2, 0.1],
    [0.1, 0.8, 0.1],
    [0.2, 0.2, 0.6]
])

# Calculate losses
binary_loss = binary_cross_entropy(y_true_binary, y_pred_binary)
categorical_loss = categorical_cross_entropy(y_true_cat, y_pred_cat)

# Display outputs
print("Binary Cross Entropy Loss:", round(binary_loss, 4))
print("Categorical Cross Entropy Loss:", round(categorical_loss, 4))

```

## Input

**Binary**
``` 
y_true_binary = [1, 0, 1, 0]
y_pred_binary = [0.9, 0.1, 0.8, 0.2]
```

**Categorical**
```
y_true_cat = [
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1]
]
y_pred_cat = [
    [0.7, 0.2, 0.1],
    [0.1, 0.8, 0.1],
    [0.2, 0.2, 0.6]
]
```

## Output

```
Binary Cross Entropy Loss: 0.1643
Categorical Cross Entropy Loss: 0.3646

```