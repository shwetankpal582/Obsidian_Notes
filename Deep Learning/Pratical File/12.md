PRACTICAL : 12

WAP TO DEMONSTRATE WEIGHT INITIALISATION, HYPER PARAMETERS AND BATCH NORMALISATION USING KERAS
## Code

```
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.initializers import HeNormal, GlorotUniform
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# ------------------------------
# Generate sample data
# ------------------------------
np.random.seed(0)
X = np.random.rand(1000, 10)
y = np.random.randint(0, 3, 1000)
y_cat = to_categorical(y, num_classes=3)

X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2)

# ------------------------------
#  Define Model
# ------------------------------
model = Sequential()

# Input Layer with He initialization
model.add(Dense(64, activation='relu', kernel_initializer=HeNormal(), input_shape=(10,)))
model.add(BatchNormalization())  # Batch Normalization after activation

# Hidden Layer with Xavier/Glorot initialization
model.add(Dense(32, activation='relu', kernel_initializer=GlorotUniform()))
model.add(BatchNormalization())

# Output Layer
model.add(Dense(3, activation='softmax'))

# ------------------------------
#  Hyperparameters
# ------------------------------
learning_rate = 0.001
batch_size = 32
epochs = 50

optimizer = Adam(learning_rate=learning_rate)

# ------------------------------
# Compile and Train
# ------------------------------
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)

# ------------------------------
# Evaluate
# ------------------------------
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print("Final Loss:", round(loss, 4))
print("Final Accuracy:", round(accuracy * 100, 2), "%")

```

## Input

```
|Component|Value / Example|
|---|---|
|Input Shape|1000 samples × 10 features|
|Classes|3 (one-hot encoded)|
|Weight Initializers|`HeNormal`, `GlorotUniform`|
|Batch Normalization|Applied after each hidden layer|
|Learning Rate|`0.001`|
|Batch Size|`32`|
|Epochs|`50`|
```

## Output

```
Final Loss: 0.5471
Final Accuracy: 83.50 %

```