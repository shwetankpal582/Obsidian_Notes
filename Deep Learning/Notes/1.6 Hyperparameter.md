Hyperparameters are **external configurations** set **before training** a model. They are **not learned from data**, but instead control the learning process itself.

They influence how well and how fast a neural network learns.

---

### 🔹 Examples of Hyperparameters

|Category|Hyperparameter|Description|
|---|---|---|
|**Optimization**|Learning Rate (η)|Step size for weight updates during training|
||Optimizer|Algorithm used (e.g., SGD, Adam, RMSProp)|
||Momentum|Helps accelerate gradients in the right direction|
|**Architecture**|Number of Layers|Depth of the network|
||Neurons per Layer|Width of each layer|
||Activation Function|ReLU, Tanh, Sigmoid, etc.|
|**Regularization**|Dropout Rate|Fraction of neurons randomly dropped|
||L1/L2 Penalty|Weight decay to avoid overfitting|
|**Training**|Batch Size|Number of samples per training step|
||Epochs|Number of full passes through the dataset|
||Weight Initialization|Starting values of weights (Xavier, He)|

---

### 🔹 Why Are Hyperparameters Important?

- 🎯 Determine **model performance** and **convergence speed**
    
- 🧠 Affect **generalization** (overfitting vs underfitting)
    
- ⚙️ Influence **training stability**
    

---

### 🔹 Techniques for Hyperparameter Tuning

|Technique|Description|
|---|---|
|**Manual Search**|Try values based on intuition or experience|
|**Grid Search**|Try all combinations from a predefined set (exhaustive)|
|**Random Search**|Randomly sample combinations (faster than grid search)|
|**Bayesian Optimization**|Use past results to choose the next best values (smart search)|
|**Hyperband / Successive Halving**|Stops bad configurations early to save time|
|**AutoML Tools**|Automatically search for best hyperparameters (e.g., AutoKeras, Optuna)|

---

### 🔹 Summary

> **Hyperparameters** are critical to how well a deep learning model trains and performs.  
> Proper tuning can dramatically improve accuracy, training speed, and generalization.