---
tags:
  - LSTM
  - DL
  - RNN
Date: 2025 - 03 - 06
Topic:
  - LSTM
Subject: DL
unit: 4
---
**LSTM (Long Short-Term Memory)** is a type of **Recurrent Neural Network (RNN)** designed to **learn from sequential data** and **remember long-term dependencies** better than traditional RNNs.

It solves the **vanishing gradient problem** faced by standard RNNs, making it effective for tasks involving time or sequence data (e.g., text, speech, time-series).

---

### 🔹 Key Components of LSTM

LSTM introduces a **memory cell** and **three gates** that control the flow of information:

1. **Forget Gate**  
    Decides **what information to discard** from the cell state.
    
    ft=σ(Wf⋅[ht−1,xt]+bf)f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)ft​=σ(Wf​⋅[ht−1​,xt​]+bf​)
2. **Input Gate**  
    Decides **what new information to store** in the cell.
    
    it=σ(Wi⋅[ht−1,xt]+bi)i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)it​=σ(Wi​⋅[ht−1​,xt​]+bi​) C~t=tanh⁡(WC⋅[ht−1,xt]+bC)\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)C~t​=tanh(WC​⋅[ht−1​,xt​]+bC​)
3. **Cell State Update**  
    Updates the **internal memory**.
    
    Ct=ft∗Ct−1+it∗C~tC_t = f_t * C_{t-1} + i_t * \tilde{C}_tCt​=ft​∗Ct−1​+it​∗C~t​
4. **Output Gate**  
    Controls what to output from the current cell.
    
    ot=σ(Wo⋅[ht−1,xt]+bo)o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)ot​=σ(Wo​⋅[ht−1​,xt​]+bo​) ht=ot∗tanh⁡(Ct)h_t = o_t * \tanh(C_t)ht​=ot​∗tanh(Ct​)

---

### 🔹 Why LSTM is Better Than RNN?

|Feature|RNN|LSTM|
|---|---|---|
|Memory|Short-term only|Long-term + short-term|
|Gradient Flow|Vanishing/Exploding|Stable (via gating mechanism)|
|Sequence Understanding|Limited|Strong for long dependencies|
|Applications|Simple sequences|Complex sequences (e.g., language, music, time-series)|

---

### 🔹 Applications of LSTM

- Text generation
    
- Sentiment analysis
    
- Speech recognition
    
- Language translation
    
- Stock price prediction
    
- Anomaly detection in time-series
    

---

### 🔹 Summary

> **LSTM** is a powerful neural network architecture designed to **remember long-term patterns** in sequential data. It uses **gates** to control memory and effectively handles problems that traditional RNNs cannot.