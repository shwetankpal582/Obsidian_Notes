---
tags:
  - LSTM
  - DL
  - RNN
Date: 2025 - 03 - 06
Topic:
  - LSTM
Subject: DL
unit: 4
---
**LSTM (Long Short-Term Memory)** is a type of **Recurrent Neural Network (RNN)** designed to **learn from sequential data** and **remember long-term dependencies** better than traditional RNNs.

It solves the **vanishing gradient problem** faced by standard RNNs, making it effective for tasks involving time or sequence data (e.g., text, speech, time-series).

---

### ðŸ”¹ Key Components of LSTM

LSTM introduces a **memory cell** and **three gates** that control the flow of information:

1. **Forget Gate**  
    Decides **what information to discard** from the cell state.
    
    ft=Ïƒ(Wfâ‹…[htâˆ’1,xt]+bf)f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)ftâ€‹=Ïƒ(Wfâ€‹â‹…[htâˆ’1â€‹,xtâ€‹]+bfâ€‹)
2. **Input Gate**  
    Decides **what new information to store** in the cell.
    
    it=Ïƒ(Wiâ‹…[htâˆ’1,xt]+bi)i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)itâ€‹=Ïƒ(Wiâ€‹â‹…[htâˆ’1â€‹,xtâ€‹]+biâ€‹) C~t=tanhâ¡(WCâ‹…[htâˆ’1,xt]+bC)\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)C~tâ€‹=tanh(WCâ€‹â‹…[htâˆ’1â€‹,xtâ€‹]+bCâ€‹)
3. **Cell State Update**  
    Updates the **internal memory**.
    
    Ct=ftâˆ—Ctâˆ’1+itâˆ—C~tC_t = f_t * C_{t-1} + i_t * \tilde{C}_tCtâ€‹=ftâ€‹âˆ—Ctâˆ’1â€‹+itâ€‹âˆ—C~tâ€‹
4. **Output Gate**  
    Controls what to output from the current cell.
    
    ot=Ïƒ(Woâ‹…[htâˆ’1,xt]+bo)o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)otâ€‹=Ïƒ(Woâ€‹â‹…[htâˆ’1â€‹,xtâ€‹]+boâ€‹) ht=otâˆ—tanhâ¡(Ct)h_t = o_t * \tanh(C_t)htâ€‹=otâ€‹âˆ—tanh(Ctâ€‹)

---

### ðŸ”¹ Why LSTM is Better Than RNN?

|Feature|RNN|LSTM|
|---|---|---|
|Memory|Short-term only|Long-term + short-term|
|Gradient Flow|Vanishing/Exploding|Stable (via gating mechanism)|
|Sequence Understanding|Limited|Strong for long dependencies|
|Applications|Simple sequences|Complex sequences (e.g., language, music, time-series)|

---

### ðŸ”¹ Applications of LSTM

- Text generation
    
- Sentiment analysis
    
- Speech recognition
    
- Language translation
    
- Stock price prediction
    
- Anomaly detection in time-series
    

---

### ðŸ”¹ Summary

> **LSTM** is a powerful neural network architecture designed to **remember long-term patterns** in sequential data. It uses **gates** to control memory and effectively handles problems that traditional RNNs cannot.