The **Softmax function** converts the output of a neural network (a vector of raw scores called _logits_) into **probabilities** that **sum to 1**.

### ğŸ”¹ Formula:

Softmax(zi)=eziâˆ‘j=1nezj\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}Softmax(ziâ€‹)=âˆ‘j=1nâ€‹ezjâ€‹eziâ€‹â€‹

Where:

- ziz_iziâ€‹: score (logit) for class iii
    
- nnn: number of classes
    

### ğŸ”¹ Key Properties:

- Output is a **probability distribution**.
    
- **Used in the final layer** of a classification model for multi-class tasks.
    
- Makes the output interpretable as probabilities.
    

---

## ğŸ”¸ **2. Cross-Entropy Loss**

### ğŸ”¹ What is Cross-Entropy?

**Cross-Entropy** measures the difference between two probability distributions:

- The **predicted** probabilities (from Softmax)
    
- The **true** class labels (as one-hot encoded vectors)
    

### ğŸ”¹ Formula:

CrossEntropy(y,y^)=âˆ’âˆ‘i=1nyiâ‹…logâ¡(y^i)\text{CrossEntropy}(y, \hat{y}) = - \sum_{i=1}^{n} y_i \cdot \log(\hat{y}_i)CrossEntropy(y,y^â€‹)=âˆ’i=1âˆ‘nâ€‹yiâ€‹â‹…log(y^â€‹iâ€‹)

Where:

- yiy_iyiâ€‹: true label (1 for correct class, 0 otherwise)
    
- y^i\hat{y}_iy^â€‹iâ€‹: predicted probability for class iii
    

### ğŸ”¹ Simplified for one-hot labels:

If true class is class 3:

Loss=âˆ’logâ¡(y^3)\text{Loss} = -\log(\hat{y}_3)Loss=âˆ’log(y^â€‹3â€‹)

---

## ğŸ” **How They Work Together**

1. **Softmax** turns the output logits into predicted probabilities.
    
2. **Cross-Entropy** compares those predicted probabilities with the actual labels to calculate how "wrong" the prediction is.
    
3. The network uses **Backpropagation** to adjust weights and reduce this loss.
    

---

## ğŸ§  **Why Use Them?**

|Concept|Purpose|
|---|---|
|Softmax|Outputs interpretable probabilities|
|Cross-Entropy|Penalizes wrong predictions based on confidence|
|Combined Use|Ideal for training **multi-class classifiers**|

---

## ğŸ”¹ Summary

- **Softmax** = Converts scores to class probabilities.
    
- **Cross-Entropy** = Measures how well the predicted probabilities match the true class.
    
- Together, they are **the standard choice** for classification problems in deep learning.