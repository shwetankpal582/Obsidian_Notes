The **Softmax function** converts the output of a neural network (a vector of raw scores called _logits_) into **probabilities** that **sum to 1**.

### 🔹 Formula:

Softmax(zi)=ezi∑j=1nezj\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}Softmax(zi​)=∑j=1n​ezj​ezi​​

Where:

- ziz_izi​: score (logit) for class iii
    
- nnn: number of classes
    

### 🔹 Key Properties:

- Output is a **probability distribution**.
    
- **Used in the final layer** of a classification model for multi-class tasks.
    
- Makes the output interpretable as probabilities.
    

---

## 🔸 **2. Cross-Entropy Loss**

### 🔹 What is Cross-Entropy?

**Cross-Entropy** measures the difference between two probability distributions:

- The **predicted** probabilities (from Softmax)
    
- The **true** class labels (as one-hot encoded vectors)
    

### 🔹 Formula:

CrossEntropy(y,y^)=−∑i=1nyi⋅log⁡(y^i)\text{CrossEntropy}(y, \hat{y}) = - \sum_{i=1}^{n} y_i \cdot \log(\hat{y}_i)CrossEntropy(y,y^​)=−i=1∑n​yi​⋅log(y^​i​)

Where:

- yiy_iyi​: true label (1 for correct class, 0 otherwise)
    
- y^i\hat{y}_iy^​i​: predicted probability for class iii
    

### 🔹 Simplified for one-hot labels:

If true class is class 3:

Loss=−log⁡(y^3)\text{Loss} = -\log(\hat{y}_3)Loss=−log(y^​3​)

---

## 🔁 **How They Work Together**

1. **Softmax** turns the output logits into predicted probabilities.
    
2. **Cross-Entropy** compares those predicted probabilities with the actual labels to calculate how "wrong" the prediction is.
    
3. The network uses **Backpropagation** to adjust weights and reduce this loss.
    

---

## 🧠 **Why Use Them?**

|Concept|Purpose|
|---|---|
|Softmax|Outputs interpretable probabilities|
|Cross-Entropy|Penalizes wrong predictions based on confidence|
|Combined Use|Ideal for training **multi-class classifiers**|

---

## 🔹 Summary

- **Softmax** = Converts scores to class probabilities.
    
- **Cross-Entropy** = Measures how well the predicted probabilities match the true class.
    
- Together, they are **the standard choice** for classification problems in deep learning.