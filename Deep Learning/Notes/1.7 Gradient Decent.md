**Gradient Descent** is an optimization algorithm used in deep learning (and many areas of machine learning) to minimize a **loss function** by updating the modelâ€™s parameters (weights and biases) iteratively.

---

## ðŸ”¹ Core Idea

The idea is simple:

> Move the model parameters in the direction of the **negative gradient** of the loss function to reduce the error.

Imagine you're on a mountain and want to reach the lowest valley (minimum loss). You take steps downhill (gradient) until you can't go any lower.

---

## ðŸ”¹ Formula

For a parameter Î¸\thetaÎ¸ (e.g., a weight), the update rule is:

Î¸=Î¸âˆ’Î±â‹…âˆ‚Lâˆ‚Î¸\theta = \theta - \alpha \cdot \frac{\partial L}{\partial \theta}Î¸=Î¸âˆ’Î±â‹…âˆ‚Î¸âˆ‚Lâ€‹

Where:

- Î¸\thetaÎ¸ = parameter (e.g., weight)
    
- Î±\alphaÎ± = **learning rate** (how big the step is)
    
- âˆ‚Lâˆ‚Î¸\frac{\partial L}{\partial \theta}âˆ‚Î¸âˆ‚Lâ€‹ = gradient of the loss with respect to Î¸\thetaÎ¸
    

---

## ðŸ”¹ Types of Gradient Descent

### 1. **Batch Gradient Descent**

- Uses the **entire dataset** to compute gradients.
    
- Very accurate but slow for large datasets.
    

### 2. **Stochastic Gradient Descent (SGD)**

- Uses **one sample at a time**.
    
- Fast and noisy (can jump around the loss surface).
    

### 3. **Mini-Batch Gradient Descent**

- Uses a **small batch of data** at each step.
    
- Trade-off between speed and accuracy.
    
- Most commonly used in practice.
    

---

## ðŸ”¹ Advanced Variants

Modern deep learning uses optimized versions of gradient descent:

- **Momentum** â€“ accelerates convergence by considering past gradients.
    
- **RMSProp** â€“ adjusts learning rate for each parameter adaptively.
    
- **Adam (Adaptive Moment Estimation)** â€“ combines Momentum + RMSProp (very popular).
    

---

## ðŸ”¹ Visual Summary

Gradient Descent steps:

1. Initialize weights randomly
    
2. Forward pass: calculate predictions
    
3. Compute loss using a loss function
    
4. Backpropagation: compute gradients
    
5. Update weights using gradient descent
    
6. Repeat steps 2â€“5 for many epochs
