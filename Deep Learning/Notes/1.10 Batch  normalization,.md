**Batch Normalization** is a technique to **normalize the inputs** of each layer in a neural network **during training**. It helps stabilize and speed up the training process.

It works by:

- **Standardizing** the activations in a mini-batch (zero mean, unit variance),
    
- Then **scaling and shifting** them using learnable parameters.
    

---

### 🔹 Why Use Batch Normalization?

BatchNorm solves problems that occur during training:

- ✅ **Reduces Internal Covariate Shift**: Makes the distribution of inputs more stable across layers.
    
- ✅ **Improves Gradient Flow**: Helps prevent vanishing/exploding gradients.
    
- ✅ **Allows Higher Learning Rates**: Leads to faster convergence.
    
- ✅ **Acts as Regularization**: Slight noise introduced by batch stats can reduce overfitting.
    

---

### 🔹 How BatchNorm Works (Steps):

For each mini-batch:

1. **Compute Mean and Variance**:
    
    μ=1m∑i=1mxi,σ2=1m∑i=1m(xi−μ)2\mu = \frac{1}{m} \sum_{i=1}^{m} x_i \quad , \quad \sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2μ=m1​i=1∑m​xi​,σ2=m1​i=1∑m​(xi​−μ)2
2. **Normalize**:
    
    x^i=xi−μσ2+ϵ\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}x^i​=σ2+ϵ​xi​−μ​
3. **Scale and Shift (Learnable)**:
    
    yi=γx^i+βy_i = \gamma \hat{x}_i + \betayi​=γx^i​+β

Where:

- ϵ\epsilonϵ is a small constant to avoid division by zero.
    
- γ\gammaγ, β\betaβ are **learned** parameters.
    

---

### 🔹 Where to Use BatchNorm?

- Typically **after the linear transformation** (e.g., dense or conv layer) and **before the activation** (like ReLU).
    
- Widely used in **CNNs, MLPs**, and **transformers**.
    

---

### 🔹 Benefits Summary

|Benefit|Explanation|
|---|---|
|📉 Faster Training|Normalized inputs help stabilize learning|
|🧠 Better Gradient Flow|Prevents issues like vanishing gradients|
|🎯 Regularization Effect|Acts like dropout to reduce overfitting|
|⚙️ Hardware Optimization|Works well with GPUs in mini-batch computation|

---

### 🔹 Summary

> **Batch Normalization** helps your neural network **train faster, more reliably, and generalize better** by normalizing activations at each layer.