An **activation function** is a mathematical function applied to the output of each neuron in a neural network. It helps the network **learn complex patterns** by introducing **non-linearity**.

Without activation functions, a deep neural network would behave like a simple linear model, no matter how many layers it has.

---

### 🔹 Why Activation Functions Are Important:

- 🔄 **Introduce non-linearity**: Allow networks to approximate complex functions and decision boundaries.
    
- 💡 **Control neuron firing**: Decide whether a neuron should activate or not.
    
- ⚡ **Enable deep learning**: Make it possible to train deep networks with multiple layers.
    

---

### 🔹 Common Activation Functions

|Name|Formula|Range|Key Features|Used In|
|---|---|---|---|---|
|**Sigmoid**|σ(x)=11+e−x\sigma(x) = \frac{1}{1 + e^{-x}}σ(x)=1+e−x1​|(0, 1)|Smooth, used for probability output|Output layer (binary classification)|
|**Tanh**|tanh⁡(x)=ex−e−xex+e−x\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}tanh(x)=ex+e−xex−e−x​|(−1, 1)|Zero-centered, better than sigmoid|Hidden layers|
|**ReLU**|f(x)=max⁡(0,x)f(x) = \max(0, x)f(x)=max(0,x)|[0, ∞)|Fast, avoids vanishing gradient|Most hidden layers|
|**Leaky ReLU**|f(x)=xf(x) = xf(x)=x if x>0x>0x>0, else αx\alpha xαx|(−∞, ∞)|Fixes ReLU’s dying neuron problem|When ReLU fails|
|**Softmax**|exi∑exj\frac{e^{x_i}}{\sum e^{x_j}}∑exj​exi​​|(0, 1) (sums to 1)|Converts outputs to probabilities over classes|Output layer (multiclass classification)|

---

### 🔹 Sigmoid vs Tanh vs ReLU — Comparison

|Feature|Sigmoid|Tanh|ReLU|
|---|---|---|---|
|Output Range|0 to 1|-1 to 1|0 to ∞|
|Zero-centered|❌|✅|❌|
|Vanishing Gradients|✅ (Yes)|✅ (less)|❌ (for x > 0)|
|Speed|Slow|Slow|Fast|
|Usage|Older networks|Output smoothing|Deep networks|

---

### 🔹 ReLU: The Most Popular

- ✅ **No vanishing gradient** for positive values.
    
- ✅ **Computationally efficient** (simple max operation).
    
- ✅ **Sparse activations** (many zeros), helping with generalization.
    
- ⚠️ **Drawback**: Some neurons can "die" (output zero always) → use Leaky ReLU.
    

---

### 🔹 Summary

- Activation functions are essential to make neural networks powerful and useful.
    
- **ReLU** is most widely used in hidden layers due to speed and effectiveness.
    
- **Sigmoid** and **tanh** are used in outputs or shallower layers.
    
- Choosing the right activation function is critical for model performance.