An **activation function** is a mathematical function applied to the output of each neuron in a neural network. It helps the network **learn complex patterns** by introducing **non-linearity**.

Without activation functions, a deep neural network would behave like a simple linear model, no matter how many layers it has.

---

### ğŸ”¹ Why Activation Functions Are Important:

- ğŸ”„ **Introduce non-linearity**: Allow networks to approximate complex functions and decision boundaries.
    
- ğŸ’¡ **Control neuron firing**: Decide whether a neuron should activate or not.
    
- âš¡ **Enable deep learning**: Make it possible to train deep networks with multiple layers.
    

---

### ğŸ”¹ Common Activation Functions

|Name|Formula|Range|Key Features|Used In|
|---|---|---|---|---|
|**Sigmoid**|Ïƒ(x)=11+eâˆ’x\sigma(x) = \frac{1}{1 + e^{-x}}Ïƒ(x)=1+eâˆ’x1â€‹|(0, 1)|Smooth, used for probability output|Output layer (binary classification)|
|**Tanh**|tanhâ¡(x)=exâˆ’eâˆ’xex+eâˆ’x\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}tanh(x)=ex+eâˆ’xexâˆ’eâˆ’xâ€‹|(âˆ’1, 1)|Zero-centered, better than sigmoid|Hidden layers|
|**ReLU**|f(x)=maxâ¡(0,x)f(x) = \max(0, x)f(x)=max(0,x)|[0, âˆ)|Fast, avoids vanishing gradient|Most hidden layers|
|**Leaky ReLU**|f(x)=xf(x) = xf(x)=x if x>0x>0x>0, else Î±x\alpha xÎ±x|(âˆ’âˆ, âˆ)|Fixes ReLUâ€™s dying neuron problem|When ReLU fails|
|**Softmax**|exiâˆ‘exj\frac{e^{x_i}}{\sum e^{x_j}}âˆ‘exjâ€‹exiâ€‹â€‹|(0, 1) (sums to 1)|Converts outputs to probabilities over classes|Output layer (multiclass classification)|

---

### ğŸ”¹ Sigmoid vs Tanh vs ReLU â€” Comparison

|Feature|Sigmoid|Tanh|ReLU|
|---|---|---|---|
|Output Range|0 to 1|-1 to 1|0 to âˆ|
|Zero-centered|âŒ|âœ…|âŒ|
|Vanishing Gradients|âœ… (Yes)|âœ… (less)|âŒ (for x > 0)|
|Speed|Slow|Slow|Fast|
|Usage|Older networks|Output smoothing|Deep networks|

---

### ğŸ”¹ ReLU: The Most Popular

- âœ… **No vanishing gradient** for positive values.
    
- âœ… **Computationally efficient** (simple max operation).
    
- âœ… **Sparse activations** (many zeros), helping with generalization.
    
- âš ï¸ **Drawback**: Some neurons can "die" (output zero always) â†’ use Leaky ReLU.
    

---

### ğŸ”¹ Summary

- Activation functions are essential to make neural networks powerful and useful.
    
- **ReLU** is most widely used in hidden layers due to speed and effectiveness.
    
- **Sigmoid** and **tanh** are used in outputs or shallower layers.
    
- Choosing the right activation function is critical for model performance.