---
tags:
  - RNN
  - DL
Date: 2025 - 03 - 06
Topic:
  - RNN
Subject: DL
unit: 4
---
## Recurrent Neural Network

In neural network the information flows in one direction from input to output. However in RNN information is fed back into the system after each step. 

RNNs allow the network to "remember" past information by feeding the output from one step into next step. This helps the network understand the context of what has already happened and make better predictions based on that. 

For example 
	when predicting the next word in a sentence the RNN uses the previous words to help decide what word is most likely to come next.
	Think of it like reading a sentence, when you're trying to predict the next word you don’t just look at the current word but also need to remember the words that came before to make accurate guess.

## How RNN Differs from Feedforward Neural Networks?

Feedforward Neural Networks (FNNs) 
	process data in one direction from input to output without retaining information from previous inputs. This makes them suitable for tasks with independent inputs like image classification. However FNNs struggle with sequential data since they lack memory.

Recurrent Neural Networks (RNNs) 
	solve this by incorporating loops that allow information from previous steps to be fed back into the network. This feedback enables RNNs to remember prior inputs making them ideal for tasks where context is important

![[Pasted image 20250518200416.png]]

## Key Components of RNN

### 1. Recurrent Neurons
### 2. RNN Unfolding

### 1. Recurrent Neurons
Recurrent units hold a hidden state that maintains information about previous inputs in a sequence. Recurrent units can "remember" information from prior steps by feeding back their hidden state, allowing them to capture dependencies across time.

![[Pasted image 20250518202053.png]]
### 2. RNN Unfolding

the process of expanding the recurrent structure over time steps. During unfolding each step of the sequence is represented as a separate layer in a series illustrating how information flows across each time step.

This unrolling enables backpropagation through time (BPTT). A learning process where errors are propagated across time steps to adjust the network’s weights enhancing the RNN’s ability to learn dependencies within sequential data.

![[Pasted image 20250518202141.png]]

## Types Of Recurrent Neural Networks

- ### One-to-One RNN
- ### One-to-Many RNN
- ### Many-to-One RNN
- ### Many-to-Many RNN

### 1. One-to-One RNN
This is the simplest type of neural network architecture where there is a single input and a single output. It is used for straightforward classification tasks such as binary classification where no sequential data is involved.

![One-to-One](https://media.geeksforgeeks.org/wp-content/uploads/20231204131135/One-to-One-300.webp)

### 2. One-to-Many RNN
RNN the network processes a single input to produce multiple outputs over time. This is useful in tasks where one input triggers a sequence of predictions (outputs). 
	For example in image captioning a single image can be used as input to generate a sequence of words as a caption.

![One-to-Many](https://media.geeksforgeeks.org/wp-content/uploads/20231204131304/One-to-Many-300.webp)

### 3. Many-to-One RNN
The Many-to-One RNN receives a sequence of inputs and generates a single output. This type is useful when the overall context of the input sequence is needed to make one prediction. In sentiment analysis the model receives a sequence of words (like a sentence) and produces a single output like positive, negative or neutral.

![Many-to-One](https://media.geeksforgeeks.org/wp-content/uploads/20231204131355/Many-to-One-300.webp)

Many to One RNN

### 4. Many-to-Many RNN
The Many-to-Many RNN type processes a sequence of inputs and generates a sequence of outputs. In language translation task a sequence of words in one language is given as input, and a corresponding sequence in another language is generated as output.

![Many-to-Many](https://media.geeksforgeeks.org/wp-content/uploads/20231204131436/Many-to-Many-300.webp)
