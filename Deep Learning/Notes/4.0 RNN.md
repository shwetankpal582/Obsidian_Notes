---
tags:
  - RNN
  - DL
Date: 2025 - 03 - 06
Topic:
  - RNN
Subject: DL
unit: 4
---
## Recurrent Neural Network

In neural network the information flows in one direction from input to output. However in RNN information is fed back into the system after each step. 

RNNs allow the network to "remember" past information by feeding the output from one step into next step. This helps the network understand the context of what has already happened and make better predictions based on that. 

For example 
	when predicting the next word in a sentence the RNN uses the previous words to help decide what word is most likely to come next.
	Think of it like reading a sentence, when you're trying to predict the next word you don’t just look at the current word but also need to remember the words that came before to make accurate guess.

## How RNN Differs from Feedforward Neural Networks?

Feedforward Neural Networks (FNNs) 
	process data in one direction from input to output without retaining information from previous inputs. This makes them suitable for tasks with independent inputs like image classification. However FNNs struggle with sequential data since they lack memory.

Recurrent Neural Networks (RNNs) 
	solve this by incorporating loops that allow information from previous steps to be fed back into the network. This feedback enables RNNs to remember prior inputs making them ideal for tasks where context is important

![[Pasted image 20250518200416.png]]

## Key Components of RNN

### 1. Recurrent Neurons
### 2. RNN Unfolding

### 1. Recurrent Neurons
Recurrent units hold a hidden state that maintains information about previous inputs in a sequence. Recurrent units can "remember" information from prior steps by feeding back their hidden state, allowing them to capture dependencies across time.

![[Pasted image 20250518202053.png]]
### 2. RNN Unfolding

the process of expanding the recurrent structure over time steps. During unfolding each step of the sequence is represented as a separate layer in a series illustrating how information flows across each time step.

This unrolling enables backpropagation through time (BPTT) a learning process where errors are propagated across time steps to adjust the network’s weights enhancing the RNN’s ability to learn dependencies within sequential data.