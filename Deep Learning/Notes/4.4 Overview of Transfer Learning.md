**Transfer Learning** is a deep learning technique where a model trained on one task is **reused** (partially or fully) for a different but related task.

---

### ðŸ”¹ Why Use Transfer Learning?

- Saves **time** and **computational resources**
    
- Works well with **limited data**
    
- Leverages **knowledge** from large datasets (like ImageNet)
    

---

### ðŸ”¹ Key Concept

Instead of training a model from scratch:

- Use a **pretrained model** (e.g., ResNet, VGG, BERT)
    
- **Transfer** its learned features (like edges, shapes, or language patterns)
    
- Fine-tune or retrain the final layers for your specific task
    

---

### ðŸ”¹ Types of Transfer Learning

1. **Feature Extraction**
    
    - Freeze all layers of the pretrained model
        
    - Only train the last few layers (classifier)
        
2. **Fine-Tuning**
    
    - Unfreeze some of the top layers of the pretrained model
        
    - Retrain with a low learning rate on your dataset
        

---

### ðŸ”¹ Common Pretrained Models

- **Computer Vision:** ResNet, VGG, MobileNet, EfficientNet
    
- **NLP:** BERT, GPT, RoBERTa, T5
    

---

### ðŸ”¹ Applications

- Image classification (with limited labeled data)
    
- Sentiment analysis using pretrained language models
    
- Medical diagnosis using pretrained models on general image datasets