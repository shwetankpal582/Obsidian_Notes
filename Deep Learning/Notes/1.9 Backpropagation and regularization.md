### What is Backpropagation?

Backpropagation is the **learning algorithm** used in neural networks to **update weights** by calculating how much each weight contributes to the overall error.

It's based on **the chain rule of calculus**, and it works by **propagating the error backward** from the output layer to the input layer.

---

### 🔹 Steps in Backpropagation

1. **Forward Pass**
    
    - Input is passed through the network to compute output (prediction).
        
2. **Loss Calculation**
    
    - The difference between the predicted output and the true label is calculated using a **loss function** (e.g., MSE, Cross-Entropy).
        
3. **Backward Pass (Gradient Computation)**
    
    - Compute **gradients of the loss** with respect to each weight using the **chain rule**.
        
4. **Weight Update**
    
    - Update weights using **Gradient Descent**:
        
        w:=w−η⋅∂L∂ww := w - \eta \cdot \frac{\partial L}{\partial w}w:=w−η⋅∂w∂L​
        
        where η = learning rate, and L = loss function.
        

---

### 🔹 Importance of Backpropagation

- Core algorithm behind training deep neural networks.
    
- Efficiently updates all weights to **minimize the loss**.
    
- Enables networks to **learn complex mappings** from input to output.
    

---

## 🛡️ **Regularization in Deep Learning**

### 🔹 What is Regularization?

Regularization is a technique used to **prevent overfitting** by discouraging the model from learning overly complex patterns or memorizing the training data.

It works by adding a **penalty term** to the loss function to **control the size of model parameters**.

---

### 🔹 Common Regularization Techniques

|Technique|Description|Use Case|
|---|---|---|
|**L1 (Lasso)**|Adds sum of absolute weights to the loss function (sparsity)|Feature selection|
|**L2 (Ridge)**|Adds sum of squared weights to the loss function (weight shrinkage)|Most common in deep learning|
|**Dropout**|Randomly drops neurons during training|Prevents co-adaptation of nodes|
|**Early Stopping**|Stops training when validation loss starts increasing|Avoids over-training|
|**Data Augmentation**|Increases training data artificially (e.g., rotate, flip images)|Helps generalization|
|**Weight Decay**|Another name for L2 regularization, common in frameworks like PyTorch|Common practice|

---

### 🔹 Loss with Regularization

Regularized Loss=Original Loss+λ⋅Penalty Term\text{Regularized Loss} = \text{Original Loss} + \lambda \cdot \text{Penalty Term}Regularized Loss=Original Loss+λ⋅Penalty Term

Where:

- λ\lambdaλ is the **regularization strength** (hyperparameter).
    
- Penalty term depends on L1 or L2 norms.
    

---

### 🔹 Summary

- **Backpropagation** trains the network by computing and applying gradients to minimize the loss.
    
- **Regularization** improves **generalization** by preventing overfitting.
    
- Both are **essential components** of modern deep learning systems.