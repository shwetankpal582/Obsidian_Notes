---
tags:
  - DL
  - Auto_Encoders
Date: 2025 - 03 - 06
Topic:
  - Auto Encoders
  - DL
Subject: DL
unit: 3
---
An autoencoder is a type of neural network architecture used in Deep Learning for unsupervised learning and dimensionality reduction tasks. It is primarily designed to learn efficient representations of data by encoding it into a lower-dimensional laten space and then decoding it back to the original data space. Autoencoders are widely used to various applications, including data compression, denoising, feature learning, and anamaly detection.

**Encoders :**
	Used for data Compression
	After compression data is converted into lower dimension
	Compressed data is called Encoded Data

**Decoders :** 
	Used for reconstruct compressed data

**Visual Representation**

![[Pasted image 20250517040328.png]]

### 1. Architecture

Autoencoders consist of main two parts :
	- An Encoder
	- A Decoder

**1.1 Encoder :** 
The encoder network takes the input data and maps it to a lower-dimensional representation, often called the encoding or latent space. This encoding is typically a compressed representation of the input data. The encoder can consist of one or more hidden layers and user activation functions like ReLu (Rectified Linear Unit).

**1.2 Decoder :**
The decoder network takes the encoded representation and attempts to reconstruct the original input data from it. Like the encoder, the decoder can also have one or more hidden layers and used activation function.

**2 Objective :**
The main objective of an autoencoder is to minimize the reconstruction error, which measures how well the decoder can reconstruct the input data from the encoding. Common loss functions used for this purpose include mean squared error (MSE) or binary cross entropy, depending on the type of data (continuous or binary) and the specific task.

**3 Training :**
Autoencoders are trained in an unsupervised manner, which means they don't require labeled data. The training process involves feeding input data into the encoder, encoding it into a lower dimensional representation, and then decoding it back to the original data. The difference between the input and the reconstructed output is used to compute the loss, which is minimized during training using optimization techniques like gradient decent.

**4 Variations of Autoencoders :**
- Denoising Autoencoder
- Sparse Autoencoder
- Variational Autoencoder
- Convolutional Autoencoder
- Recurrent Autoencoder

**Denoising  Autoencoder :**
	Trained to remove noise from data by corrupting the input and reconstructing the clean data.

**Sparse Autoencoder :**
	Encourages the network to learn sparse representation which can be useful to feature learning and dimensionality reduction.

**Variational Autoencoder :**
	Introduces probabilistic elements in the latent space, allowing for generation of new data samples and improved data compression.

**Convolutional Autoencoder :**
	Specifically designed for image data, using convolutional layers in the encoder and decoder.

**Recurrent Autoencoder :**
	Suitable for sequential data, such as time series, by using recurrent layers in the architecture.
	
**Working**
- Input Data
- Encoder
- Decoder
- Loss Function

**Input Data :**
	Autoencoders take numerical data as input. This data could be anything from images (pixel values), audio (amplitude values over time), text (word embedding), or any form of numerical representation.

**Encoder :**
	The encoder network receives this numerical data and maps it to a lower-dimensional numerical representation. 
		**For example :**
			In case of Image data : 
				the encoder might transform pixel values into a reduced set of numerical features that capture essential information.

**Decoder :**
	The decoder takes the reduced numerical representation and attempts to reconstruct the original numerical data. It does so by mapping the reduced representation back to the original numerical format.

**Loss Function :**
	During training, the difference between the input numerical data and the reconstructed data us measured using a numerical loss function. The model then adjusts its weights and biases through numerical optimization techniques to minimize this loss.

**Summery**
	Autoencoders are a versatile tool in deep learning for learning compact representation of data and have a numerous applications in various domains, working with both numerical and non-numerical data.